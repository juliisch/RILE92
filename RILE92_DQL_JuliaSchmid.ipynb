{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Digital Business University of Applied Sciences\n",
    "\n",
    "Data Science und Management (M. Sc.)\n",
    "\n",
    "RILE92 SP IV-2: Reinforcement Learning\n",
    "\n",
    "Claudia Baldermann\n",
    "\n",
    "Julia Schmid (200022)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "#    Deep Q-Learning (DQN)\n",
    "***\n",
    "\n",
    "In diesem Jupyter-Notebook wird das Deep-Q-Learning (DQN) Verfahren in der Cliff Walking Umgebung von OpenAI (2022) trainiert und evaluiert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://gymnasium.farama.org/_images/cliff_walking.gif\" width=\"400\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Image\n",
    "Image(url='https://gymnasium.farama.org/_images/cliff_walking.gif',width=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Import der notwendigen Pakete**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verwendung der cuda-fähigen CPU (falls vorhanden)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hyperparameter setzten**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter\n",
    "gamma = 0.99            # Diskontierungsfaktor\n",
    "alpha = 0.005           # Lernrate\n",
    "num_episodes = 2500     # Anzahl der Episoden\n",
    "\n",
    "epsilon = 1.0           # Epsilon\n",
    "epsilon_final = 0.1     # minmales Epsilon \n",
    "epsilon_decay = 0.0002  # Epsilon Reduzierungsfaktor\n",
    "\n",
    "batch_size = 50         # Größe der Stichproben\n",
    "buffer_size = 75        # Größe des Replay Buffers\n",
    "\n",
    "N = 40                  # Aktualisierungsfaktor für das Target-Netzwork"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reinforcement Umgebung definieren**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anzahl der Zustände: 48\n",
      "Anzahl der Aktionen: 4\n"
     ]
    }
   ],
   "source": [
    "# Environment definieren (Cliff Walking)\n",
    "env = gym.make('CliffWalking-v0')\n",
    "\n",
    "# Bestimmung der Anzahl der möglichen Zustände\n",
    "state_size = env.observation_space.n # 48 Zustände\n",
    "print(f\"Anzahl der Zustände: {state_size}\")\n",
    "\n",
    "# Bestimmung der Anzahl der möglichen Aktionen\n",
    "action_size = env.action_space.n # 4 Aktionen \n",
    "print(f\"Anzahl der Aktionen: {action_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Funktionen für das DQN definieren**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funktion: Definierung eines neuronalen Netzes mit Pytorch\n",
    "# Input: state_size (Anzahl der möglichen Zustände), action_size (Anzahl der möglichen Aktionen) \n",
    "# Output: Q-Werte\n",
    "# Funktionsweise: Es wird ein neuronales Netz für die Bestimmung der Q-Werte definiert, welches aus einer Eingabeschicht (Anzahl der Zustände), einer versteckten Schickt mit 64 Neurononen und einer Ausgabeschickt (Anzahl der Aktion) besteht. Es wird die Aktivierungsfunktion ReLu angewendet. \n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, state_size, action_size):\n",
    "        super().__init__()\n",
    "        self.hidden1 = nn.Linear(state_size, 128)\n",
    "        self.hidden2 = nn.Linear(128, 128)\n",
    "        self.output = nn.Linear(128, action_size)\n",
    "\n",
    "    def forward(self, s):\n",
    "        x = F.relu(self.hidden1(s))\n",
    "        x = F.relu(self.hidden2(x))\n",
    "        return self.output(x)\n",
    "    \n",
    "# Quelle: Paszke (2024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funktion: Auwahl einer Aktion nach der Epsilon-Greedy-Strategie\n",
    "# Input: state (aktueller Zustand), q_network (Q-Netzwerk), epsilon (Epsilon)\n",
    "# Output: action (Aktion)\n",
    "# Funktionsweise: Per Zufall (np.random.rand()) wird entschieden, ob Exploration (Auswahl einer zufälligen Aktion) oder Exploitation (Auswahl der besten Aktion) gewählt wird.\n",
    "\n",
    "def select_action(state, q_network, epsilon):\n",
    "    if np.random.rand() <= epsilon: # Exploration\n",
    "        action=  env.action_space.sample()  # Auswahl einer zufällige Aktion\n",
    "        return action\n",
    "    else: # Exploitation\n",
    "        with torch.no_grad(): # Deaktivierung der automatischen Berechnung von Gradienten\n",
    "            state_tensor = torch.eye(state_size)[state].unsqueeze(0) # Umwandlung der State in Vektor (für neuronales Netz)\n",
    "            q_values = q_network(state_tensor)  # Q-Wert bestimmen\n",
    "            action = torch.argmax(q_values).item() # Auwahl der Aktion mit höchsten Q-Wert (besten Aktion)\n",
    "            return action\n",
    "\n",
    "# Quelle: In Anlehnung an Géron (2019)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funktion: Trainingsprozess\n",
    "# Input: replay_buffer (Replay Buffer), qNetwork_model (Q-Netzwerk), targetNetwork_model (Target-Netzwerk), optimizer (Optimizer für neuronale Netze), batch_size (Größe der Stichprobe), state_size (Anzahl der möglichen Zustände)\n",
    "# Output: loss.item() (Verlustwert)\n",
    "# Funktionsweise: Es wird eine zufällige Stichprobe (Batch) aus dem Replay Buffer gezogen. Basierend darauf werden die Q-Werte vom Q-Netzwerk und Target-Netzwerk bestimmt. Mit diesen Werten wird der Verlustwert (Loss-Wert) bestimmt. \n",
    "\n",
    "def train(replay_buffer, qNetwork_model, targetNetwork_model, optimizer, batch_size, state_size):\n",
    "    replay_buffer_batch = random.sample(replay_buffer, batch_size) # Es wird ein zufällige Stichprobe (Batch) aus dem Replay-Buffer gezogen\n",
    "    states, actions, rewards, state_next, dones = zip(*replay_buffer_batch)  # Speicherung der einzelnen Komponenten aus dem Replay-Buffer-Batch \n",
    "    # Speicherung der Komponenten im richtigen Format\n",
    "    states = torch.eye(state_size)[list(states)]\n",
    "    actions = torch.tensor(actions)\n",
    "    state_next = torch.eye(state_size)[list(state_next)]\n",
    "    rewards = torch.tensor(rewards, dtype=torch.float32)\n",
    "    dones = torch.tensor([float(done) for done in dones], dtype=torch.float32)\n",
    "\n",
    "    # Verwendung des Q-Netzwerks und des Target-Netzwerks um die Loss-Funktion zu bestimmen:  \n",
    "    q_values = qNetwork_model(states).gather(1, actions.unsqueeze(1)).squeeze() # Bestimmung der Q-Werte aus dem Q-Netzwerk \n",
    "    next_q_values = targetNetwork_model(state_next).max(1)[0].detach() # Bestimmung des besten Q-Wertes aus dem Target-Netzwerk\n",
    "    target_q_values = rewards + gamma * next_q_values * (1 - dones) # Zielwert\n",
    "\n",
    "    loss = F.mse_loss(q_values, target_q_values) # Bestimmung des Verlustes (loss) mittels MSE \n",
    "\n",
    "    optimizer.zero_grad() # Löschung der Gradienten\n",
    "    loss.backward() # Bestimmung der Gradienten\n",
    "    optimizer.step() # Aktualisierung der Modellparameter\n",
    "\n",
    "    return loss.item()\n",
    "\n",
    "# Quelle: In Anlehnung an Géron (2019)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**DQN Anwenden auf definierte Umgebung**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/juliaschmid/anaconda3/envs/ADS/lib/python3.11/site-packages/gym/utils/passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 2499 of 2500, Reward: -761, Epsilon: 0.500, Loss avg.: 23.318854\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "reward_records = [] # Leere Liste für die Speicherung der erhaltenen gesamten Belohnungen (Rewards) - Für die Evaluation\n",
    "loss_records = [] # Leere Liste für die Speicherung der durchschnittlichen Loss Werte pro Episode - Für die Evaluation\n",
    "\n",
    "qNetwork = NeuralNetwork(state_size, action_size) # Inizialisierung des Q-Netzwerks \n",
    "targetNetwork = NeuralNetwork(state_size, action_size) # Inizialisierung des Target-Netzwerks\n",
    "targetNetwork.load_state_dict(qNetwork.state_dict()) # Target-Nertwerk erhält die gleichen Gewichte wie das Q-Netzwerk\n",
    "optimizer = optim.Adam(qNetwork.parameters(), lr=alpha) # Adam-Optimizer, welcher die Gewichte (qNetwork.parameters()) mittels Gradientenabstiegs aktualisiert \n",
    "\n",
    "# Inizialisierung des Replay-Buffers\n",
    "replay_buffer = deque(maxlen=buffer_size) \n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    # Initialisierung\n",
    "    state = env.reset()[0]  \n",
    "    total_reward = 0\n",
    "    total_loss = 0\n",
    "    count_loss = 0\n",
    "    done = False  \n",
    "        \n",
    "    while not done:\n",
    "        action = select_action(state, qNetwork, epsilon)  # Es wird eine Aktion gewählt (epsilon-Greedy)\n",
    "        state_next, reward, done, _, _ = env.step(action)  # Es wird die ausgewähle Aktion action (a_t) ausgeführt und basierend auf der ausgeführten Aktion der nächste Zustand state_next (s_{t+1}), die Belohnung reward (r_{t+1}) und der Done-Zustand done gespeichert\n",
    "\n",
    "        replay_buffer.append((state, action, reward, state_next, done))  # Zustand, Aktion, Belohnung und nächster Zustand werden im Replay-Buffer gespeichert\n",
    "        \n",
    "        state = state_next # Aktualisierung des aktuellen Zustands (Fortbewegung im Spielfeld)\n",
    "        total_reward += reward  # Aktualisierung der gesamten Belohnungen \n",
    "            \n",
    "        ## Training beginnt erst, wenn eine Anzahl von Erfahrungen (batch_size) gesammelt worden sind\n",
    "        if len(replay_buffer) < batch_size:\n",
    "            continue\n",
    "        # Sobald genügend Erfahrungen gesammelt werden, beginnt das Training\n",
    "        else:\n",
    "            loss = train(replay_buffer, qNetwork, targetNetwork, optimizer, batch_size, state_size)\n",
    "            if loss is not None:\n",
    "                total_loss += loss\n",
    "                count_loss += 1\n",
    "    \n",
    "    reward_records.append(total_reward) # Speicherung der gesamten Belohnungen\n",
    "\n",
    "                \n",
    "    # Alle N Schritte wernde die Gewichte des Target-Nertwerks aktualisiert\n",
    "    if episode % N == 0:\n",
    "        targetNetwork.load_state_dict(qNetwork.state_dict())\n",
    "\n",
    "    # Berechnung des durchschnittlichen Verlusts pro Episode\n",
    "    loss_averange = 0\n",
    "    if count_loss > 0:\n",
    "        loss_records.append(total_loss / count_loss)\n",
    "        loss_averange = total_loss / count_loss\n",
    "    else:\n",
    "        loss_records.append(0)\n",
    "        loss_averange = 0\n",
    "\n",
    "    # Aktualisierung von Epsilon\n",
    "    epsilon = max(epsilon_final, epsilon - epsilon_decay)\n",
    "                \n",
    "    print(f\"Episode {episode} of {num_episodes}, Reward: {total_reward}, Epsilon: {epsilon:.3f}, Loss avg.: {loss_averange:.3f}\", end=\"\\r\")\n",
    "\n",
    "env.close()\n",
    "print(\"\\nDone\")\n",
    "\n",
    "# Quelle: In Anlehnung an Géron (2019) und Ravichandiran (2018)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Evaluierung**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected exception formatting exception. Falling back to standard exception\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/Users/juliaschmid/anaconda3/envs/ADS/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n",
      "  File \"/var/folders/jf/k1y_jzxs5177jm_fgv__dnww0000gn/T/ipykernel_95657/312320484.py\", line 2, in <module>\n",
      "    plt.figure(figsize=(12, 5))\n",
      "  File \"/Users/juliaschmid/anaconda3/envs/ADS/lib/python3.11/site-packages/matplotlib/pyplot.py\", line 934, in figure\n",
      "  File \"/Users/juliaschmid/anaconda3/envs/ADS/lib/python3.11/site-packages/matplotlib/pyplot.py\", line 464, in new_figure_manager\n",
      "  File \"/Users/juliaschmid/anaconda3/envs/ADS/lib/python3.11/site-packages/matplotlib/pyplot.py\", line 441, in _warn_if_gui_out_of_main_thread\n",
      "  File \"/Users/juliaschmid/anaconda3/envs/ADS/lib/python3.11/site-packages/matplotlib/pyplot.py\", line 280, in _get_backend_mod\n",
      "  File \"/Users/juliaschmid/anaconda3/envs/ADS/lib/python3.11/site-packages/matplotlib/pyplot.py\", line 303, in switch_backend\n",
      "ModuleNotFoundError: No module named 'matplotlib.backends'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/juliaschmid/anaconda3/envs/ADS/lib/python3.11/site-packages/pygments/styles/__init__.py\", line 89, in get_style_by_name\n",
      "ModuleNotFoundError: No module named 'pygments.styles.default'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/juliaschmid/anaconda3/envs/ADS/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 2144, in showtraceback\n",
      "  File \"/Users/juliaschmid/anaconda3/envs/ADS/lib/python3.11/site-packages/IPython/core/ultratb.py\", line 1435, in structured_traceback\n",
      "  File \"/Users/juliaschmid/anaconda3/envs/ADS/lib/python3.11/site-packages/IPython/core/ultratb.py\", line 1326, in structured_traceback\n",
      "  File \"/Users/juliaschmid/anaconda3/envs/ADS/lib/python3.11/site-packages/IPython/core/ultratb.py\", line 1173, in structured_traceback\n",
      "  File \"/Users/juliaschmid/anaconda3/envs/ADS/lib/python3.11/site-packages/IPython/core/ultratb.py\", line 1063, in format_exception_as_a_whole\n",
      "  File \"/Users/juliaschmid/anaconda3/envs/ADS/lib/python3.11/site-packages/IPython/core/ultratb.py\", line 1114, in get_records\n",
      "  File \"/Users/juliaschmid/anaconda3/envs/ADS/lib/python3.11/site-packages/pygments/styles/__init__.py\", line 91, in get_style_by_name\n",
      "pygments.util.ClassNotFound: Could not find style module 'default', though it should be builtin.\n"
     ]
    }
   ],
   "source": [
    "# Plot Rewards pro Episode und Loss pro Episode\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Reward pro Episode \n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(range(len(reward_records)), reward_records, label=\"Reward\")\n",
    "# Trendlinie einfügen\n",
    "poly_trend = np.poly1d(np.polyfit(range(len(reward_records)), reward_records, 3))\n",
    "plt.plot(np.arange(len(reward_records)), poly_trend(np.arange(len(reward_records))), label=\"Trendlinie\", color=\"orange\")\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Reward\")\n",
    "plt.title(\"Reward pro Episode\")\n",
    "plt.legend(loc=\"upper center\", bbox_to_anchor=(0.5, -0.15), ncol=2)\n",
    "\n",
    "# Verlust pro Episode\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(range(len(loss_records)), loss_records, label=\"Loss\", color='red')\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Loss pro Episode\")\n",
    "plt.legend(loc=\"upper center\", bbox_to_anchor=(0.5, -0.15), ncol=2)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ADS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
